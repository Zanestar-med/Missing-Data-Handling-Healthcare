{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RspRs_rR0nza"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "sklearn.set_config(transform_output='pandas')\n",
    "\n",
    "FILE_PATH = \"NHANES_hypertension.pkl\"\n",
    "IT_IMP_SUBSET = 2000\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7PKqMQO0Qyl"
   },
   "source": [
    "# Assignment 3: Handling missing data\n",
    "\n",
    "In this assignment, you will explore different methods for handling missing values in the development of prediction models. You will use a data set based on the National Health and Nutrition Examination Survey (NHANES) run by the Centers for Disease Control and Prevention (CDC) in the USA: https://www.cdc.gov/nchs/nhanes/index.htm.\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "* Your goal is to predict hypertension (high blood pressure) from a number of subject covariates. \n",
    "* You will compare impute-then-regress classifiers to methods that handle missing values natively.\n",
    "\n",
    "## Data\n",
    "\n",
    "The NHANES survey is possible to download from the CDC but is spread over 100s of CSV files. For your convenience, we have compiled a .pkl file with a dataframe for this assignment. We cannot share it publicly on the web so instead...\n",
    "\n",
    "* For this assignment, you will need to download a .pkl data file from Canvas \n",
    "* Place the file with the name ```NHANES_hypertension.pkl``` in the same directory as this notebook\n",
    "* The covariates are described in the file ```NHANES_hypertension.codes.txt```, also available on Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b_kBFRdh0Oe8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>SMQ020</th>\n",
       "      <th>SMD680</th>\n",
       "      <th>SMD415</th>\n",
       "      <th>SMD415A</th>\n",
       "      <th>PAD020</th>\n",
       "      <th>PAD200</th>\n",
       "      <th>...</th>\n",
       "      <th>PAD660</th>\n",
       "      <th>PAD675</th>\n",
       "      <th>SMD460</th>\n",
       "      <th>SMDANY</th>\n",
       "      <th>BMXHIP</th>\n",
       "      <th>ALQ121</th>\n",
       "      <th>MCQ366C</th>\n",
       "      <th>BPXOSY1</th>\n",
       "      <th>BPXODI1</th>\n",
       "      <th>HYPERT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEQN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102952.0</th>\n",
       "      <td>102952.0</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>87.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102953.0</th>\n",
       "      <td>102953.0</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>112.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102954.0</th>\n",
       "      <td>102954.0</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>102.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102955.0</th>\n",
       "      <td>102955.0</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102956.0</th>\n",
       "      <td>102956.0</td>\n",
       "      <td>2017-2018</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69118 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              SEQN       YEAR  RIDAGEYR RIAGENDR SMQ020 SMD680  SMD415  \\\n",
       "SEQN                                                                     \n",
       "2.0            2.0  1999-2000      77.0      1.0    2.0    2.0     NaN   \n",
       "3.0            3.0  1999-2000      10.0      2.0    NaN    NaN     1.0   \n",
       "5.0            5.0  1999-2000      49.0      1.0    1.0    1.0     NaN   \n",
       "6.0            6.0  1999-2000      19.0      2.0    NaN    2.0     NaN   \n",
       "7.0            7.0  1999-2000      59.0      2.0    1.0    2.0     NaN   \n",
       "...            ...        ...       ...      ...    ...    ...     ...   \n",
       "102952.0  102952.0  2017-2018      70.0      2.0    2.0    2.0     0.0   \n",
       "102953.0  102953.0  2017-2018      42.0      1.0    1.0    2.0     0.0   \n",
       "102954.0  102954.0  2017-2018      41.0      2.0    2.0    2.0     0.0   \n",
       "102955.0  102955.0  2017-2018      14.0      2.0    NaN    2.0     2.0   \n",
       "102956.0  102956.0  2017-2018      38.0      1.0    1.0    1.0     1.0   \n",
       "\n",
       "          SMD415A PAD020 PAD200  ... PAD660 PAD675  SMD460  SMDANY  BMXHIP  \\\n",
       "SEQN                             ...                                         \n",
       "2.0           NaN    2.0    3.0  ...    NaN    NaN     NaN     NaN     NaN   \n",
       "3.0           1.0    NaN    NaN  ...    NaN    NaN     NaN     1.0     NaN   \n",
       "5.0           NaN    2.0    1.0  ...    NaN    NaN     NaN     NaN     NaN   \n",
       "6.0           NaN    1.0    1.0  ...    NaN    NaN     NaN     NaN     NaN   \n",
       "7.0           NaN    1.0    2.0  ...    NaN    NaN     NaN     NaN     NaN   \n",
       "...           ...    ...    ...  ...    ...    ...     ...     ...     ...   \n",
       "102952.0      0.0    NaN    NaN  ...    NaN   60.0     0.0     2.0    87.3   \n",
       "102953.0      0.0    NaN    NaN  ...    NaN    NaN     0.0     2.0   112.8   \n",
       "102954.0      0.0    NaN    NaN  ...    NaN   30.0     0.0     2.0   102.7   \n",
       "102955.0      2.0    NaN    NaN  ...    NaN    NaN     2.0     2.0   128.3   \n",
       "102956.0      1.0    NaN    NaN  ...    NaN    NaN     1.0     1.0   110.0   \n",
       "\n",
       "          ALQ121 MCQ366C BPXOSY1 BPXODI1  HYPERT  \n",
       "SEQN                                              \n",
       "2.0          NaN     NaN     NaN     NaN       0  \n",
       "3.0          NaN     NaN     NaN     NaN       0  \n",
       "5.0          NaN     NaN     NaN     NaN       0  \n",
       "6.0          NaN     NaN     NaN     NaN       0  \n",
       "7.0          NaN     NaN     NaN     NaN       0  \n",
       "...          ...     ...     ...     ...     ...  \n",
       "102952.0     0.0     2.0   154.0    92.0       0  \n",
       "102953.0     6.0     1.0   135.0    91.0       0  \n",
       "102954.0     NaN     2.0   123.0    75.0       0  \n",
       "102955.0     NaN     NaN    92.0    64.0       0  \n",
       "102956.0     8.0     2.0   143.0   100.0       1  \n",
       "\n",
       "[69118 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_full = pd.read_pickle(FILE_PATH)\n",
    "D_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 — Exploration & imputation\n",
    "\n",
    "### Data exploration & setup\n",
    "\n",
    "* The columns 'SEQN' and 'YEAR' represent the subject ID and year of survey, respectively. These should *not* be used as input for prediction.\n",
    "* The outcome column $Y$ is called 'HYPERT'\n",
    "* The columns 'BPXSY1', 'BPXDI1', 'BPXOSY1', 'BPXODI1' all measure the blood pressure and are used to compute the outcome. These should *not* be used as input for prediction. \n",
    "\n",
    "\n",
    "1. Report the frequency of missing values in each input column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of features: 43\n",
      "\n",
      "--- Frequency of missing values (First 10 columns) ---\n",
      "RIDAGEYR    0.000000\n",
      "RIAGENDR    0.000000\n",
      "SMQ020      0.303307\n",
      "SMD680      0.664299\n",
      "SMD415      0.575103\n",
      "SMD415A     0.575103\n",
      "PAD020      0.653723\n",
      "PAD200      0.653752\n",
      "PAD320      0.653838\n",
      "DIQ010      0.000666\n",
      "dtype: float64\n",
      "\n",
      "--- Task 2 Report ---\n",
      "Columns removed (>50% missing): ['SMD680', 'SMD415', 'SMD415A', 'PAD020', 'PAD200', 'PAD320', 'OHAROCDT', 'OHAROCGP', 'OHARNF', 'ALQ120Q', 'MCQ160C', 'MCQ080', 'OCQ180', 'OCQ380', 'OCD180', 'PAD645', 'PAD660', 'PAD675', 'SMD460', 'SMDANY', 'BMXHIP', 'ALQ121', 'MCQ366C']\n",
      "Number of features remaining: 20\n",
      "List of remaining columns: ['RIDAGEYR', 'RIAGENDR', 'SMQ020', 'DIQ010', 'BMXWT', 'BMXHT', 'BMXBMI', 'BMXWAIST', 'BPQ020', 'DRXTTFAT', 'DRXTSFAT', 'LBXSAL', 'LBXSGL', 'LBXSCH', 'LBXSUA', 'LBXSKSI', 'DBD100', 'DR1TTFAT', 'DR1TSFAT', 'INDFMMPI']\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define features (X) and target (y) ---\n",
    "# Define columns to exclude: Subject ID, Year, and direct blood pressure readings \n",
    "# (BPX... columns are excluded to prevent data leakage as they directly determine the outcome)\n",
    "cols_to_exclude = ['SEQN', 'YEAR', 'BPXSY1', 'BPXDI1', 'BPXOSY1', 'BPXODI1', 'HYPERT']\n",
    "\n",
    "# Define the target variable\n",
    "y = D_full['HYPERT']\n",
    "\n",
    "# Define the initial feature set X by dropping the excluded columns\n",
    "# errors='ignore' prevents errors if columns are missing or already dropped\n",
    "X_raw = D_full.drop(columns=cols_to_exclude, errors='ignore')\n",
    "\n",
    "print(f\"Initial number of features: {X_raw.shape[1]}\")\n",
    "\n",
    "# --- 2. Task 1: Report frequency of missing values ---\n",
    "# Calculate the fraction of missing values for each column (0.0 to 1.0)\n",
    "missing_fraction = X_raw.isnull().mean()\n",
    "\n",
    "print(\"\\n--- Frequency of missing values (First 10 columns) ---\")\n",
    "print(missing_fraction.head(10))\n",
    "\n",
    "# --- 3. Task 2: Remove columns with > 50% missingness ---\n",
    "# Identify columns where the missing fraction is greater than 0.5 (50%)\n",
    "cols_to_drop = missing_fraction[missing_fraction > 0.5].index.tolist()\n",
    "\n",
    "# Drop these columns from the feature set\n",
    "X_clean = X_raw.drop(columns=cols_to_drop)\n",
    "\n",
    "# Report the results\n",
    "print(\"\\n--- Task 2 Report ---\")\n",
    "print(f\"Columns removed (>50% missing): {cols_to_drop}\")\n",
    "print(f\"Number of features remaining: {X_clean.shape[1]}\")\n",
    "print(f\"List of remaining columns: {X_clean.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove columns with more than 50% missingness and report which columns you removed and which remain\n",
    "* Variable types can be found in the property ```dtypes``` of the dataframe. Categorical variables have the type 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns to encode: 5\n",
      "Numeric columns to scale: 13\n",
      "Shape after one-hot encoding: (69118, 32)\n",
      "Training set shape: (55294, 32)\n",
      "Test set shape: (13824, 32)\n",
      "Standard scaling applied to numeric features.\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Step 1: Define Categorical and Numeric Columns ---\n",
    "# Based on the remaining columns in X_clean and the codes.txt\n",
    "# We need to be precise here.\n",
    "\n",
    "# Categorical variables (Nominal/Ordinal) in the remaining set\n",
    "cat_cols = [\n",
    "    'RIAGENDR', 'SMQ020', 'DIQ010', 'BPQ020', 'DBD100', \n",
    "    'MCQ160C', 'MCQ080', 'OCQ180', 'OCQ380', 'OCD180', \n",
    "    'ALQ121', 'MCQ366', 'SMD460', 'SMDANY'\n",
    "]\n",
    "\n",
    "# Numeric variables (Continuous) in the remaining set\n",
    "num_cols = [\n",
    "    'RIDAGEYR', 'BMXWT', 'BMXHT', 'BMXBMI', 'BMXWAIST', \n",
    "    'LBXSAL', 'LBXSGL', 'LBXSCH', 'LBXSUA', 'LBXSKSI', \n",
    "    'DR1TTFAT', 'DR1TSFAT', 'INDFMMPI', 'ALQ120Q', 'BMXHIP'\n",
    "]\n",
    "\n",
    "# Filter lists to include ONLY columns that actually exist in X_clean\n",
    "# This prevents \"KeyError\" if a column was already dropped in Task 2\n",
    "final_cat_cols = [c for c in cat_cols if c in X_clean.columns]\n",
    "final_num_cols = [c for c in num_cols if c in X_clean.columns]\n",
    "\n",
    "print(f\"Categorical columns to encode: {len(final_cat_cols)}\")\n",
    "print(f\"Numeric columns to scale: {len(final_num_cols)}\")\n",
    "\n",
    "# --- Step 2: Task 3 - One-hot Encoding with NaN as a category ---\n",
    "# dummy_na=True creates a column like 'SMQ020_nan' if there are missing values\n",
    "X_encoded = pd.get_dummies(X_clean, columns=final_cat_cols, dummy_na=True, dtype=int)\n",
    "\n",
    "print(f\"Shape after one-hot encoding: {X_encoded.shape}\")\n",
    "\n",
    "# --- Step 3: Task 4 - Train/Test Split ---\n",
    "# Split data into 80% training and 20% test with random_state=0\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# --- Step 4: Task 5 - Standard Scaling ---\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the numeric features of the training set\n",
    "scaler.fit(X_train[final_num_cols])\n",
    "\n",
    "# Apply the transformation to both training and test sets\n",
    "# We use .loc to update only the numeric columns in place\n",
    "X_train.loc[:, final_num_cols] = scaler.transform(X_train[final_num_cols])\n",
    "X_test.loc[:, final_num_cols] = scaler.transform(X_test[final_num_cols])\n",
    "\n",
    "print(\"Standard scaling applied to numeric features.\")\n",
    "print(\"Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Perform one-hot encoding of categorical variables such that missing values are encoded as a separate category. For example, if a binary variable ```Test``` has values 0, 1 and missing values NaN, the categories should be ```Test_0```, ```Test_1``` and ```Test_NaN``` (although, the names are up to you)\n",
    "4. Split your data into a training portion (80%) and a test portion (20%) with random_state=0\n",
    "5. Fit a standard scaler to the numeric features in the training portion and apply that to both training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Fit a constant imputer for numeric (non-categorical) input variables on the training set using the variable *median* as constant. Categorical variables should not be imputed since they are handled by the one-hot encoding. Impute missing values both in the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Fit an IterativeImputer (akin to MICE) using scikit-learn for numeric (non-category) variables. Impute missing values both in the training and test sets. (Don't use posterior sampling here. We do single, not multiple imputation in this assignment.)\n",
    "* Since IterativeImputer is quite slow for large samples, fit the imputer to a subset of the training set of size ```IT_IMP_SUBSET```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 6: Constant (Median) Imputation complete.\n",
      "Fitting IterativeImputer on subset of size: (2000, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zijian Xin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 7: Iterative Imputation complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# --- Task 6: Constant Imputer (Median) ---\n",
    "# We create a copy of the data to avoid modifying the original X_train/X_test\n",
    "X_train_const = X_train.copy()\n",
    "X_test_const = X_test.copy()\n",
    "\n",
    "# Initialize SimpleImputer with strategy='median'\n",
    "const_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Fit on training data ONLY\n",
    "const_imputer.fit(X_train_const[final_num_cols])\n",
    "\n",
    "# Transform both training and test data\n",
    "X_train_const.loc[:, final_num_cols] = const_imputer.transform(X_train_const[final_num_cols])\n",
    "X_test_const.loc[:, final_num_cols] = const_imputer.transform(X_test_const[final_num_cols])\n",
    "\n",
    "print(\"Task 6: Constant (Median) Imputation complete.\")\n",
    "\n",
    "# --- Task 7: Iterative Imputer (MICE) ---\n",
    "# Create another copy for iterative imputation\n",
    "X_train_iter = X_train.copy()\n",
    "X_test_iter = X_test.copy()\n",
    "\n",
    "# Initialize IterativeImputer\n",
    "# random_state=0 for reproducibility\n",
    "iter_imputer = IterativeImputer(random_state=0, max_iter=10)\n",
    "\n",
    "# Fit on a SUBSET of training data (as requested by the problem to save time)\n",
    "# IT_IMP_SUBSET was defined in Setup (e.g., 2000)\n",
    "# If IT_IMP_SUBSET is not defined, we define it here just in case\n",
    "if 'IT_IMP_SUBSET' not in locals():\n",
    "    IT_IMP_SUBSET = 2000\n",
    "\n",
    "X_train_subset = X_train_iter[final_num_cols].iloc[:IT_IMP_SUBSET]\n",
    "\n",
    "print(f\"Fitting IterativeImputer on subset of size: {X_train_subset.shape}\")\n",
    "iter_imputer.fit(X_train_subset)\n",
    "\n",
    "# Transform both training and test data (using the imputer fitted on the subset)\n",
    "X_train_iter.loc[:, final_num_cols] = iter_imputer.transform(X_train_iter[final_num_cols])\n",
    "X_test_iter.loc[:, final_num_cols] = iter_imputer.transform(X_test_iter[final_num_cols])\n",
    "\n",
    "print(\"Task 7: Iterative Imputation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Evaluate both imputation strategies. \n",
    "* Since the value of missing variables is unknown, do this by: \n",
    "* Copying the test set into a new data frame\n",
    "* Adding missing values to 5% to the *observed values* of data frame selected uniformly at random (make them NaN). \n",
    "* Report the MSE on the modified subset of values, compared to the original observations, where each error is normalized by the standard deviation of the corresponding column in the training set.\n",
    "* Are the results expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificially removed 7719 values for evaluation.\n",
      "----------------------------------------\n",
      "MSE (Constant/Median Imputation): 1.0359\n",
      "MSE (Iterative/MICE Imputation):  0.4740\n",
      "----------------------------------------\n",
      "Conclusion: Iterative Imputer performed BETTER (lower error).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Task 8: Evaluate Imputation Strategies (Artificial Missingness Experiment) ---\n",
    "\n",
    "# 1. Setup: Get the numeric part of the original test set\n",
    "X_test_num = X_test[final_num_cols].copy()\n",
    "\n",
    "# 2. Create a mask for observed values (Not NaN)\n",
    "observed_mask = ~np.isnan(X_test_num.values)\n",
    "# Get the indices (row, col) of all observed values\n",
    "observed_indices = np.argwhere(observed_mask)\n",
    "\n",
    "# 3. Randomly select 5% of observed values to mask\n",
    "np.random.seed(42) # Ensure reproducibility\n",
    "n_remove = int(len(observed_indices) * 0.05) # 5%\n",
    "\n",
    "# Choose random indices\n",
    "remove_idx_indices = np.random.choice(len(observed_indices), n_remove, replace=False)\n",
    "remove_coords = observed_indices[remove_idx_indices]\n",
    "\n",
    "# 4. Create the \"Corrupted\" Test Set\n",
    "X_test_corrupted = X_test_num.copy()\n",
    "# Set selected values to NaN\n",
    "for row, col in remove_coords:\n",
    "    X_test_corrupted.iloc[row, col] = np.nan\n",
    "\n",
    "print(f\"Artificially removed {n_remove} values for evaluation.\")\n",
    "\n",
    "# 5. Impute the corrupted data using our pre-fitted imputers\n",
    "# Method A: Constant (Median)\n",
    "X_test_const_eval = const_imputer.transform(X_test_corrupted)\n",
    "# Method B: Iterative\n",
    "X_test_iter_eval = iter_imputer.transform(X_test_corrupted)\n",
    "\n",
    "# 6. Calculate MSE (Normalized)\n",
    "true_values = []\n",
    "const_preds = []\n",
    "iter_preds = []\n",
    "\n",
    "\n",
    "for row, col in remove_coords:\n",
    "  \n",
    "    true_val = X_test_num.iloc[row, col]\n",
    " \n",
    "    const_val = X_test_const_eval.iloc[row, col]\n",
    "    iter_val = X_test_iter_eval.iloc[row, col]\n",
    "    \n",
    "    true_values.append(true_val)\n",
    "    const_preds.append(const_val)\n",
    "    iter_preds.append(iter_val)\n",
    "\n",
    "\n",
    "# Calculate MSE\n",
    "mse_const = mean_squared_error(true_values, const_preds)\n",
    "mse_iter = mean_squared_error(true_values, iter_preds)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"MSE (Constant/Median Imputation): {mse_const:.4f}\")\n",
    "print(f\"MSE (Iterative/MICE Imputation):  {mse_iter:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if mse_iter < mse_const:\n",
    "    print(\"Conclusion: Iterative Imputer performed BETTER (lower error).\")\n",
    "else:\n",
    "    print(\"Conclusion: Constant Imputer performed BETTER (lower error).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "er4l_NwY0-O9"
   },
   "source": [
    "## Problem 2 — Predict hypertension\n",
    "\n",
    "In this problem, you will use the imputed data sets for predicting hypertension with column name ```HYPERT``` using the other variables as input (excluding columns removed previously). You will compare classifiers fit to the imputed data sets, and classifiers that handle missing values natively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit LogisticRegression (LR) models to the two training data sets imputed with the constant and iterative imputer, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression on Median Imputed Data...\n",
      "Training Logistic Regression on MICE Imputed Data...\n",
      "Training HistGradientBoostingClassifier on Raw Data...\n",
      "\n",
      "--- Model Performance (AUROC) ---\n",
      "1. LR + Median Imputation:   Train=0.8621, Test=0.8585\n",
      "2. LR + MICE Imputation:     Train=0.8620, Test=0.8586\n",
      "3. HGBC (Native Handling):   Train=0.8981, Test=0.8653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- Critical Fix: Ensure no NaNs remain for Logistic Regression ---\n",
    "# Logistic Regression cannot handle NaN values. \n",
    "# Although we imputed numeric columns, some NaNs might remain in categorical columns \n",
    "# (even after one-hot encoding if not perfectly handled) or edge cases.\n",
    "# We fill any remaining NaNs with 0 (safe for one-hot encoded data).\n",
    "\n",
    "X_train_const_clean = X_train_const.fillna(0)\n",
    "X_test_const_clean = X_test_const.fillna(0)\n",
    "\n",
    "X_train_iter_clean = X_train_iter.fillna(0)\n",
    "X_test_iter_clean = X_test_iter.fillna(0)\n",
    "\n",
    "# --- 1. Fit Logistic Regression on Constant (Median) Imputed Data ---\n",
    "print(\"Training Logistic Regression on Median Imputed Data...\")\n",
    "# Increased max_iter to ensure convergence\n",
    "lr_const = LogisticRegression(random_state=0, max_iter=2000) \n",
    "lr_const.fit(X_train_const_clean, y_train)\n",
    "\n",
    "# Predict probabilities for AUROC (class 1)\n",
    "y_pred_const_train = lr_const.predict_proba(X_train_const_clean)[:, 1]\n",
    "y_pred_const_test = lr_const.predict_proba(X_test_const_clean)[:, 1]\n",
    "\n",
    "# --- 2. Fit Logistic Regression on Iterative (MICE) Imputed Data ---\n",
    "print(\"Training Logistic Regression on MICE Imputed Data...\")\n",
    "lr_iter = LogisticRegression(random_state=0, max_iter=2000)\n",
    "lr_iter.fit(X_train_iter_clean, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_iter_train = lr_iter.predict_proba(X_train_iter_clean)[:, 1]\n",
    "y_pred_iter_test = lr_iter.predict_proba(X_test_iter_clean)[:, 1]\n",
    "\n",
    "# --- 3. Fit HistGradientBoostingClassifier on Unimputed Data (Native Handling) ---\n",
    "# HGBC handles missing values natively, so we use the original X_train/X_test (containing NaNs)\n",
    "print(\"Training HistGradientBoostingClassifier on Raw Data...\")\n",
    "hgbc = HistGradientBoostingClassifier(random_state=0)\n",
    "hgbc.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_hgbc_train = hgbc.predict_proba(X_train)[:, 1]\n",
    "y_pred_hgbc_test = hgbc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- 4. Report AUROC Results ---\n",
    "print(\"\\n--- Model Performance (AUROC) ---\")\n",
    "print(f\"1. LR + Median Imputation:   Train={roc_auc_score(y_train, y_pred_const_train):.4f}, Test={roc_auc_score(y_test, y_pred_const_test):.4f}\")\n",
    "print(f\"2. LR + MICE Imputation:     Train={roc_auc_score(y_train, y_pred_iter_train):.4f}, Test={roc_auc_score(y_test, y_pred_iter_test):.4f}\")\n",
    "print(f\"3. HGBC (Native Handling):   Train={roc_auc_score(y_train, y_pred_hgbc_train):.4f}, Test={roc_auc_score(y_test, y_pred_hgbc_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit an [HistGradientBoostingClassifier](https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) (HGBC) to the *unimputed* data set with missing values. The HGBC handles missing values natively by learning default rules which are used when a missing value is encountered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Report the training and test set AUROC for all models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Is imputation better than native handling of missing values in this case? Can you tell from the results you already have? If not, perform an experiment to gather more evidence. Describe this experiment, run it, and give your conclusions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment: HGBC on MICE Imputed Data...\n",
      "HGBC + MICE Imputation Test AUROC: 0.8651\n",
      "HGBC + Native Handling Test AUROC: 0.8653 (from previous step)\n",
      "Conclusion: Native handling is BETTER than imputation for HGBC.\n"
     ]
    }
   ],
   "source": [
    "# --- Experiment: Fair Comparison (HGBC on Imputed vs. HGBC on Native) ---\n",
    "# To isolate the effect of missing value handling, we must use the SAME classifier.\n",
    "# We already have HGBC on Native data (0.8653).\n",
    "# Now, let's fit HGBC on the MICE-imputed data.\n",
    "\n",
    "print(\"Running Experiment: HGBC on MICE Imputed Data...\")\n",
    "\n",
    "hgbc_imputed = HistGradientBoostingClassifier(random_state=0)\n",
    "hgbc_imputed.fit(X_train_iter, y_train)\n",
    "\n",
    "y_pred_hgbc_imp_test = hgbc_imputed.predict_proba(X_test_iter)[:, 1]\n",
    "auroc_hgbc_imp = roc_auc_score(y_test, y_pred_hgbc_imp_test)\n",
    "\n",
    "print(f\"HGBC + MICE Imputation Test AUROC: {auroc_hgbc_imp:.4f}\")\n",
    "print(f\"HGBC + Native Handling Test AUROC: {0.8653} (from previous step)\")\n",
    "\n",
    "if 0.8653 > auroc_hgbc_imp:\n",
    "    print(\"Conclusion: Native handling is BETTER than imputation for HGBC.\")\n",
    "else:\n",
    "    print(\"Conclusion: Imputation is BETTER or EQUAL to native handling for HGBC.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 4\n",
    "\n",
    "**1. Can we tell from the initial results?**\n",
    "No, we cannot strictly conclude whether imputation is better than native handling based solely on the initial results.\n",
    "*   **Reasoning:** The initial comparison was between **Logistic Regression (with imputation)** and **HGBC (with native handling)**. Since HGBC is a non-linear ensemble method, it is inherently more powerful than a linear Logistic Regression model. Therefore, the higher AUROC of HGBC (0.8653) could be due to the algorithm's superior predictive power rather than its method of handling missing values.\n",
    "\n",
    "**2. The Experiment**\n",
    "To make a fair comparison, I trained the **same classifier (HGBC)** on the **MICE-imputed data** and compared it to the **HGBC on raw data (native handling)**.\n",
    "\n",
    "**3. Results & Conclusion**\n",
    "*   **HGBC (Native Handling):** 0.8653\n",
    "*   **HGBC (MICE Imputed):** 0.8651\n",
    "\n",
    "**Conclusion:**\n",
    "Native handling is **slightly better** (or comparable) to imputation in this case.\n",
    "*   **Why?** Tree-based models like HGBC handle missing values natively by learning the optimal direction (left or right child) for NaN values at each split. This allows the model to treat \"missingness\" as informative information itself.\n",
    "*   **Implication:** Imputation, even advanced methods like MICE, forces a specific value onto the missing entry. In this case, the computational cost of imputation does not yield performance benefits over the model's native handling capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 — Complete-case analysis\n",
    "\n",
    "In this problem, you will compare classifiers fit only to complete cases to methods aimed at overcoming missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit complete-case classifiers (LR and HGBC) by dropping all rows with missing values in your selected input columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compare your classifiers fit to the full data sets (imputed and with missing values) to the complete-case classifiers on the test set, restricted to complete cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What can you say about the performance of your complete-case models on the full population? Would the results from the complete-case subset transfer (for example, if the missing values in the full population were observed)? \n",
    "\n",
    "4. Under what conditions do results from complete-case analysis generally transfer to a population with missingness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Investigate how the complete cases differ from the overall population. Can you see substantial differences in distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training Set Size: 55294\n",
      "Complete-Case Training Set Size: 22815\n",
      "Dropped 32479 rows (58.7%) due to missingness.\n",
      "\n",
      "Training models on Complete Cases only...\n",
      "Testing on 5701 complete cases from the test set.\n",
      "\n",
      "--- Comparison on Complete Cases (Test Set) ---\n",
      "LR (Trained on CC):   0.8257\n",
      "LR (Trained on Full): 0.8249\n",
      "HGBC (Trained on CC):   0.8297\n",
      "HGBC (Trained on Full): 0.8303\n",
      "\n",
      "--- Task 5: Distribution Differences (Full vs. CC) ---\n",
      "Hypertension Prevalence (Full Population): 0.1615 (16.1%)\n",
      "Hypertension Prevalence (Complete Cases):  0.1715 (17.2%)\n",
      "Observation: Substantial difference in target distribution detected.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- Task 1: Fit complete-case classifiers ---\n",
    "\n",
    "# 1. Identify indices of complete cases (rows with NO missing values) in Training set\n",
    "# We use the original X_train (which has NaNs)\n",
    "train_cc_indices = X_train.dropna().index\n",
    "X_train_cc = X_train.loc[train_cc_indices]\n",
    "y_train_cc = y_train.loc[train_cc_indices]\n",
    "\n",
    "print(f\"Original Training Set Size: {X_train.shape[0]}\")\n",
    "print(f\"Complete-Case Training Set Size: {X_train_cc.shape[0]}\")\n",
    "print(f\"Dropped {X_train.shape[0] - X_train_cc.shape[0]} rows ({100*(1 - len(X_train_cc)/len(X_train)):.1f}%) due to missingness.\\n\")\n",
    "\n",
    "# 2. Fit models on Complete Cases (CC)\n",
    "print(\"Training models on Complete Cases only...\")\n",
    "lr_cc = LogisticRegression(random_state=0, max_iter=2000)\n",
    "lr_cc.fit(X_train_cc, y_train_cc)\n",
    "\n",
    "hgbc_cc = HistGradientBoostingClassifier(random_state=0)\n",
    "hgbc_cc.fit(X_train_cc, y_train_cc)\n",
    "\n",
    "# --- Task 2: Compare classifiers on Test Set (Restricted to Complete Cases) ---\n",
    "\n",
    "# 1. Identify complete cases in Test set\n",
    "test_cc_indices = X_test.dropna().index\n",
    "X_test_cc = X_test.loc[test_cc_indices]\n",
    "y_test_cc = y_test.loc[test_cc_indices]\n",
    "\n",
    "print(f\"Testing on {len(X_test_cc)} complete cases from the test set.\\n\")\n",
    "\n",
    "# 2. Evaluate NEW models (Trained on CC)\n",
    "y_pred_lr_cc = lr_cc.predict_proba(X_test_cc)[:, 1]\n",
    "y_pred_hgbc_cc = hgbc_cc.predict_proba(X_test_cc)[:, 1]\n",
    "\n",
    "auc_lr_cc = roc_auc_score(y_test_cc, y_pred_lr_cc)\n",
    "auc_hgbc_cc = roc_auc_score(y_test_cc, y_pred_hgbc_cc)\n",
    "\n",
    "# 3. Evaluate OLD models (Trained on Full Data)\n",
    "# We use the models from Problem 2: lr_iter (MICE) and hgbc (Native)\n",
    "# Note: For lr_iter, we need the imputed version of X_test_cc. \n",
    "# Since X_test_cc has no NaNs, X_test_iter.loc[test_cc_indices] is effectively the same.\n",
    "y_pred_lr_old = lr_iter.predict_proba(X_test_iter.loc[test_cc_indices])[:, 1]\n",
    "y_pred_hgbc_old = hgbc.predict_proba(X_test_cc)[:, 1]\n",
    "\n",
    "auc_lr_old = roc_auc_score(y_test_cc, y_pred_lr_old)\n",
    "auc_hgbc_old = roc_auc_score(y_test_cc, y_pred_hgbc_old)\n",
    "\n",
    "print(\"--- Comparison on Complete Cases (Test Set) ---\")\n",
    "print(f\"LR (Trained on CC):   {auc_lr_cc:.4f}\")\n",
    "print(f\"LR (Trained on Full): {auc_lr_old:.4f}\")\n",
    "print(f\"HGBC (Trained on CC):   {auc_hgbc_cc:.4f}\")\n",
    "print(f\"HGBC (Trained on Full): {auc_hgbc_old:.4f}\")\n",
    "\n",
    "# --- Task 5: Investigate Distribution Differences ---\n",
    "print(\"\\n--- Task 5: Distribution Differences (Full vs. CC) ---\")\n",
    "# Compare the prevalence of Hypertension (Target y)\n",
    "prev_full = y_train.mean()\n",
    "prev_cc = y_train_cc.mean()\n",
    "\n",
    "print(f\"Hypertension Prevalence (Full Population): {prev_full:.4f} ({prev_full*100:.1f}%)\")\n",
    "print(f\"Hypertension Prevalence (Complete Cases):  {prev_cc:.4f} ({prev_cc*100:.1f}%)\")\n",
    "\n",
    "if abs(prev_full - prev_cc) > 0.01:\n",
    "    print(\"Observation: Substantial difference in target distribution detected.\")\n",
    "else:\n",
    "    print(\"Observation: Target distribution is relatively similar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers to Problem 3\n",
    "\n",
    "**3. Performance on the full population**\n",
    "*   **Performance:** Although the complete-case (CC) models perform similarly to the full models when tested *on complete cases*, their performance on the **full population** would likely be unreliable and biased.\n",
    "*   **Transferability:** The results would **NOT** transfer well. The CC model is trained on a specific subset of the population (only ~41% of the original data) that has no missing values. This subset is likely systematically different from the population with missing data. If the missing values in the full population were observed, the CC model might fail to predict them accurately because it never learned from \"incomplete\" patients during training.\n",
    "\n",
    "**4. Conditions for transferability**\n",
    "*   Results from complete-case analysis generally transfer to the full population with missingness **only if** the data is **Missing Completely At Random (MCAR)**.\n",
    "*   **MCAR** means that the probability of a data point being missing is entirely unrelated to any observed or unobserved data. In medical datasets like NHANES, this is rarely the case (e.g., sicker patients might have more complete records due to more frequent doctor visits, or conversely, very sick patients might drop out). Since the data is likely **MAR (Missing At Random)** or **MNAR (Missing Not At Random)**, removing incomplete rows introduces **selection bias**.\n",
    "\n",
    "**5. Distribution Differences**\n",
    "*   **Observation:** As shown in the code output, there is a substantial difference in the target distribution. The prevalence of hypertension in the **Complete Cases (17.2%)** is **higher** than in the **Full Population (16.1%)**.\n",
    "*   **Conclusion:** This indicates that the complete cases are **not representative** of the overall population. Individuals with hypertension are slightly *more likely* to have complete data (possibly because they undergo more rigorous medical testing and monitoring), while healthy individuals might skip certain tests, leading to missing values.\n",
    "*   **Impact:** This confirms that the missingness is **not random**. Using only complete cases introduces a bias, causing the model to overestimate the prevalence of hypertension compared to the general population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of AI: The language fluency and professionalism of the answer section have been polished using ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
